{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02785c7b-8d6c-4552-aaeb-ece5f7eba903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, random, numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch, torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d44fee0-1019-4d4a-8376-1371cf30dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import content_encoder_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24685cd0-8313-4831-b65b-9371203489f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109100b10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 1337\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19083f8f-1d0d-464b-a987-9f9294045733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#where data is stored\n",
    "base_path = \"/Users/arjun/Documents/NUS/CS5647/accentdb/accentdb_extended/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624e782b-a5a0-4990-84d5-eb9703e3edb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5396 1356 5214\n"
     ]
    }
   ],
   "source": [
    "train_set, validation_set, test_set = utils.scan_and_split_by_accent(base_path)\n",
    "print(len(train_set), len(validation_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4a77a6-1884-4383-bd5f-8332e7488622",
   "metadata": {},
   "outputs": [],
   "source": [
    "accents = sorted({lab for _, lab in train_set})\n",
    "label2id = {lab:i for i,lab in enumerate(accents)}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60fb01dc-ea4a-405c-adc5-4e0fdd1728fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american', 'australian', 'bangla', 'british', 'indian', 'malayalam', 'odiya', 'telugu', 'welsh']\n",
      "{'american': 0, 'australian': 1, 'bangla': 2, 'british': 3, 'indian': 4, 'malayalam': 5, 'odiya': 6, 'telugu': 7, 'welsh': 8}\n"
     ]
    }
   ],
   "source": [
    "print(accents)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16fd315-99bd-4089-8ce4-634fd8b3685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline hubert for initial experimentation (not required to run)\n",
    "bundle = torchaudio.pipelines.HUBERT_BASE\n",
    "hubert = bundle.get_model().eval()   # outputs [B, T', 768]\n",
    "\n",
    "@torch.no_grad()\n",
    "def ssl_embed(wav16):  # wav16: [B,1,T] or [B,T] or [1,T] @16k mono\n",
    "    x = wav16.squeeze(1) if wav16.dim()==3 and wav16.size(1)==1 else wav16\n",
    "    if x.dim()==1: x = x.unsqueeze(0)\n",
    "    feats, _ = hubert(x, None)      # [B, T', 768]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d035345-a8e6-4d75-acd7-7fa2184c3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, X, y, bs=256):\n",
    "    model.train()\n",
    "    idx = torch.randperm(X.size(0))\n",
    "    total = 0.0\n",
    "    for i in range(0, len(idx), bs):\n",
    "        b = idx[i:i+bs]\n",
    "        xb, yb = X[b], y[b]\n",
    "        loss = F.cross_entropy(model(xb), yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        total += loss.item() * xb.size(0)\n",
    "    return total / len(idx)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, X, y, labels, tag=\"VAL\"):\n",
    "    model.eval()\n",
    "    pred = model(X).argmax(1).numpy()\n",
    "    true = y.numpy()\n",
    "    acc = accuracy_score(true, pred)\n",
    "    print(f\"[{tag}] ACC: {acc:.4f}\")\n",
    "    try:\n",
    "        print(classification_report(true, pred, target_names=labels, digits=4))\n",
    "        print(\"Confusion:\\n\", confusion_matrix(true, pred))\n",
    "    except Exception: pass\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "242faf6c-2a74-4904-a40d-8a03e9b9b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SR = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3d46f73-b5a5-4b8c-b585-17fed2138e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import path variables for loading contentvec model\n",
    "PROJECT_ROOT = \"/Users/arjun/Lab/cs5647-labs/project_cv\"\n",
    "\n",
    "# 1. Update CONTENTVEC_ROOT to point to the Fairseq clone directory\n",
    "# This directory contains the installed Fairseq package and the custom code.\n",
    "CONTENTVEC_ROOT = os.path.join(PROJECT_ROOT, \"contentvec\", \"fairseq\") \n",
    "\n",
    "# 2. Update MODEL_PATH as needed\n",
    "MODEL_PATH = os.path.join(PROJECT_ROOT, \"model\", \"checkpoint_best_500.pt\")\n",
    "#MODEL_PATH = os.path.join(PROJECT_ROOT, \"model\", \"checkpoint_best_legacy_500.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffbbb5b5-62fb-4246-8917-16a8862d728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentVecEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.out_dim = 768\n",
    "\n",
    "        # Load the ContentVec / HuBERT model from checkpoint\n",
    "        models, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task(\n",
    "        [MODEL_PATH], \n",
    "        arg_overrides={\n",
    "            \"data\": os.path.dirname(MODEL_PATH),\n",
    "            \"user_dir\": CONTENTVEC_ROOT, # Tells Fairseq where to find the custom code\n",
    "        }\n",
    "        )\n",
    "        self.model = models[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract(self, wav16):  # [1, T] or [B, T]\n",
    "        self.model.eval()\n",
    "\n",
    "        x = wav16\n",
    "        # convert [B,1,T] → [B,T]\n",
    "        if x.dim() == 3 and x.size(1) == 1:\n",
    "            x = x.squeeze(1)\n",
    "        # convert [T] → [1,T]\n",
    "        elif x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        \n",
    "        out = self.model(\n",
    "            x,                   # Your main audio input\n",
    "            #source_2=None,              # Pass None for the second audio view\n",
    "            #spk_emb=None,               # Pass None for speaker embedding\n",
    "            features_only=True, \n",
    "            mask=False\n",
    "        )\n",
    "\n",
    "        return out[\"x\"]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_layer(self, wav16, layer_idx):\n",
    "        x = wav16\n",
    "        if x.dim() == 3 and x.size(1) == 1:\n",
    "            x = x.squeeze(1)\n",
    "        elif x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        out = self.model(\n",
    "            x,\n",
    "            features_only=True,\n",
    "            mask=False,\n",
    "            output_layer=layer_idx + 1,   # <-- IMPORTANT\n",
    "        )\n",
    "\n",
    "        return out[\"x\"] \n",
    "    @torch.no_grad()\n",
    "    def extract_with_embeddings(self, wav16, spk_emb_input=None):  # [1, T] or [B, T], spk_emb_input: [B, 192]\n",
    "        self.model.eval()\n",
    "\n",
    "        x = wav16\n",
    "        # Convert audio input to [B, T]\n",
    "        if x.dim() == 3 and x.size(1) == 1:\n",
    "            x = x.squeeze(1)\n",
    "        elif x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "        B = x.size(0)\n",
    "        SPK_EMB_DIM = 192 # CONFIRMED DIMENSION\n",
    "        \n",
    "        # Use the provided speaker embedding if available, otherwise create a zero tensor\n",
    "        if spk_emb_input is not None:\n",
    "            # Ensure the provided embedding is correctly shaped\n",
    "            if spk_emb_input.dim() == 1:\n",
    "                 # Case: [192] -> [1, 192]\n",
    "                spk_emb = spk_emb_input.unsqueeze(0)\n",
    "            elif spk_emb_input.dim() == 2:\n",
    "                # Case: [B, 192]\n",
    "                spk_emb = spk_emb_input\n",
    "            else:\n",
    "                raise ValueError(f\"spk_emb_input must be [192] or [B, 192], got shape {spk_emb_input.shape}\")\n",
    "        else:\n",
    "            # Create a dummy speaker embedding tensor (all zeros) if none is provided\n",
    "            # This is what you would do if you only wanted content features\n",
    "            print(\"Generating dummy embeddings\")\n",
    "            spk_emb = torch.zeros(B, SPK_EMB_DIM, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        # Call the dedicated extraction method (the cleanest way)\n",
    "        # Note: If you want a specific intermediate layer, set output_layer=N (1-based index)\n",
    "        out, _ = self.model.extract_features(\n",
    "            source=x,\n",
    "            spk_emb=spk_emb, # Pass the correctly sized speaker embedding\n",
    "            padding_mask=None,\n",
    "            mask=False,\n",
    "            ret_conv=False,\n",
    "            output_layer=None, # Use the final layer\n",
    "            tap=False\n",
    "        )\n",
    "        \n",
    "        # 'out' is the final representation tensor: [B, T', 768]\n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f87be67e-a9ef-428d-8994-9a6ea5d42a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjun/Lab/cs5647-labs/project_cv/contentvec/fairseq/fairseq/checkpoint_utils.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "2025-11-04 15:55:05 | INFO | fairseq.tasks.contentvec_pretraining | current directory is /Users/arjun/Lab/cs5647-labs/project_cv\n",
      "2025-11-04 15:55:05 | INFO | fairseq.tasks.contentvec_pretraining | ContentvecPretrainingTask Config {'_name': 'contentvec_pretraining', 'data': '/Users/arjun/Lab/cs5647-labs/project_cv/model', 'fine_tuning': False, 'labels': ['km'], 'label_dir': 'label', 'label_rate': 50, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'crop': True, 'pad_audio': False, 'spk2info': 'spk2info.dict'}\n",
      "2025-11-04 15:55:05 | INFO | fairseq.models.hubert.contentvec | ContentvecModel Config: {'_name': 'contentvec', 'label_rate': 50, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_layers_1': 3, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'ctr_layers': [-6], 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'logit_temp_ctr': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'cross_sample_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False}\n",
      "/Users/arjun/miniconda3/envs/contentvec/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "content_encoder = ContentVecEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abbd4a8b-ac4c-4440-8fb4-45da5d693b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fairseq.models.hubert.contentvec.ContentvecModel'>\n",
      "has encoder: True\n",
      "has hubert: False\n",
      "has extract_features: True\n"
     ]
    }
   ],
   "source": [
    "cv = content_encoder.model\n",
    "print(type(cv))\n",
    "print(\"has encoder:\", hasattr(cv, \"encoder\"))\n",
    "print(\"has hubert:\", hasattr(cv, \"hubert\"))\n",
    "print(\"has extract_features:\", hasattr(cv, \"extract_features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3a9c76c-f574-4062-b817-287d0b0397dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 15:55:07 | DEBUG | speechbrain.utils.checkpoints | Registered checkpoint save hook for _speechbrain_save\n",
      "2025-11-04 15:55:07 | DEBUG | speechbrain.utils.checkpoints | Registered checkpoint load hook for _speechbrain_load\n",
      "2025-11-04 15:55:07 | DEBUG | speechbrain.utils.checkpoints | Registered checkpoint save hook for save\n",
      "2025-11-04 15:55:07 | DEBUG | speechbrain.utils.checkpoints | Registered checkpoint load hook for load\n",
      "2025-11-04 15:55:07 | INFO | speechbrain.utils.quirks | Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "2025-11-04 15:55:07 | INFO | speechbrain.utils.quirks | Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "2025-11-04 15:55:07 | DEBUG | speechbrain.utils.checkpoints | Registered checkpoint save hook for _save\n",
      "2025-11-04 15:55:07 | DEBUG | speechbrain.utils.checkpoints | Registered checkpoint load hook for _recover\n",
      "/Users/arjun/Lab/cs5647-labs/project_cv/speechEncoder.py:2: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n"
     ]
    }
   ],
   "source": [
    "import speechEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22023b87-1a69-42c1-a09e-14ac7c74f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 15:55:09 | INFO | speechbrain.utils.fetching | Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "2025-11-04 15:55:09 | INFO | speechbrain.utils.fetching | Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "2025-11-04 15:55:10 | DEBUG | speechbrain.utils.checkpoints | Registered checkpoint save hook for _save\n",
      "2025-11-04 15:55:10 | DEBUG | speechbrain.utils.checkpoints | Registered checkpoint load hook for _load\n",
      "2025-11-04 15:55:10 | DEBUG | speechbrain.utils.checkpoints | Registered parameter transfer hook for _load\n",
      "/Users/arjun/miniconda3/envs/contentvec/lib/python3.8/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "2025-11-04 15:55:10 | DEBUG | speechbrain.utils.checkpoints | Registered checkpoint save hook for save\n",
      "2025-11-04 15:55:10 | DEBUG | speechbrain.utils.checkpoints | Registered checkpoint load hook for load_if_possible\n",
      "2025-11-04 15:55:10 | DEBUG | speechbrain.utils.parameter_transfer | Fetching files for pretraining (no collection directory set)\n",
      "2025-11-04 15:55:10 | INFO | speechbrain.utils.fetching | Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "2025-11-04 15:55:10 | DEBUG | speechbrain.utils.parameter_transfer | Set local path in self.paths[\"embedding_model\"] = /Users/arjun/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/embedding_model.ckpt\n",
      "2025-11-04 15:55:10 | INFO | speechbrain.utils.fetching | Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "2025-11-04 15:55:10 | DEBUG | speechbrain.utils.parameter_transfer | Set local path in self.paths[\"mean_var_norm_emb\"] = /Users/arjun/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/mean_var_norm_emb.ckpt\n",
      "2025-11-04 15:55:10 | INFO | speechbrain.utils.fetching | Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "2025-11-04 15:55:11 | DEBUG | speechbrain.utils.parameter_transfer | Set local path in self.paths[\"classifier\"] = /Users/arjun/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/classifier.ckpt\n",
      "2025-11-04 15:55:11 | INFO | speechbrain.utils.fetching | Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "2025-11-04 15:55:11 | DEBUG | speechbrain.utils.parameter_transfer | Set local path in self.paths[\"label_encoder\"] = /Users/arjun/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n",
      "2025-11-04 15:55:11 | INFO | speechbrain.utils.parameter_transfer | Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "2025-11-04 15:55:11 | DEBUG | speechbrain.utils.parameter_transfer | Redirecting (loading from local path): embedding_model -> /Users/arjun/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/embedding_model.ckpt\n",
      "2025-11-04 15:55:11 | DEBUG | speechbrain.utils.parameter_transfer | Redirecting (loading from local path): mean_var_norm_emb -> /Users/arjun/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/mean_var_norm_emb.ckpt\n",
      "2025-11-04 15:55:11 | DEBUG | speechbrain.utils.parameter_transfer | Redirecting (loading from local path): classifier -> /Users/arjun/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/classifier.ckpt\n",
      "2025-11-04 15:55:11 | DEBUG | speechbrain.utils.parameter_transfer | Redirecting (loading from local path): label_encoder -> /Users/arjun/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n",
      "/Users/arjun/miniconda3/envs/contentvec/lib/python3.8/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/Users/arjun/miniconda3/envs/contentvec/lib/python3.8/site-packages/speechbrain/processing/features.py:1529: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "2025-11-04 15:55:11 | DEBUG | speechbrain.dataio.encoder | Loaded categorical encoding from /Users/arjun/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n"
     ]
    }
   ],
   "source": [
    "s_encoder = speechEncoder.SpeakerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21366787-3f4f-413c-876c-f8fe92824226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cached_split(pairs, label2id, encoder, speaker_encoder, cache_dir, layer = -1):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    X, y = [], []\n",
    "    for path, label in tqdm(pairs, desc=f\"Extracting to {cache_dir}\"):\n",
    "        # stable key per file\n",
    "        key = os.path.splitext(os.path.basename(path))[0]\n",
    "        npy = os.path.join(cache_dir, key + \".npy\")\n",
    "        if os.path.exists(npy):\n",
    "            emb = np.load(npy)\n",
    "        else:\n",
    "            wav = utils.load_and_resample_16k(path)          # 22k -> 16k here\n",
    "            speaker_emb = speaker_encoder.extract_embedding(path, wav, TARGET_SR)\n",
    "            speaker_emb = torch.from_numpy(speaker_emb)\n",
    "            speaker_emb = F.pad(speaker_emb, (0, 256-192), 'constant', 0)\n",
    "            with torch.no_grad():\n",
    "                if layer >= 0:\n",
    "                    feats = encoder.extract_layer(wav,layer)\n",
    "                else:\n",
    "                    feats = encoder.extract_with_embeddings(wav, speaker_emb)           # [1, T', 768]\n",
    "                    \n",
    "            #pooled = pool_mean_std(feats).cpu().numpy() # [1536]\n",
    "            np.save(npy, feats)\n",
    "            emb = feats\n",
    "        X.append(emb)\n",
    "        y.append(label2id[label])\n",
    "    #X = torch.tensor(np.stack(X), dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c9a2e0-443c-4b67-a93e-2a14c6dd8ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting to cache_cv/with_s_embs_no_pool/test: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5214/5214 [00:04<00:00, 1286.40it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data, test_label = build_cached_split(test_set, label2id, content_encoder, s_encoder, cache_dir=\"cache_cv/with_s_embs_no_pool/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa6ee1b1-f137-4bdb-996f-3690d96053bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting to cache_cv/with_s_embs_no_pool/train: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1356/1356 [00:01<00:00, 1238.50it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_data, validation_label = build_cached_split(validation_set, label2id, content_encoder, s_encoder, cache_dir=\"cache_cv/with_s_embs_no_pool/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57789921-20b8-4ce1-ab71-556858bad1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting to cache_cv/with_s_embs_no_pool/train: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5396/5396 [00:05<00:00, 1015.28it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label = build_cached_split(train_set, label2id, content_encoder, s_encoder, cache_dir=\"cache_cv/with_s_embs_no_pool/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6accfb28-b890-412e-8d01-98318f0b8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame_features, train_frame_labels = utils.generate_frame_level_dataset(train_data, train_label, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b14457c-dd82-4a31-a2cc-cd3bd4f63701",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_frame_features, validation_frame_labels = utils.generate_frame_level_dataset(validation_data, validation_label, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94074b3b-880d-477a-a2a7-08d2d09ced81",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame_features, test_frame_labels = utils.generate_frame_level_dataset(test_data, test_label, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb55fc0c-d151-496a-b95a-50d48c6131b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([53960, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frame_features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "855dc456-4fa5-411d-9bde-b9b0404e247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakageTester(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden=None):\n",
    "        super().__init__()\n",
    "        self.net = (nn.Linear(input_dim, num_classes) if hidden is None else\n",
    "                    nn.Sequential(nn.Linear(input_dim, hidden), nn.ReLU(), nn.Dropout(0.2),\n",
    "                                  nn.Linear(hidden, num_classes)))\n",
    "    def forward(self, x): \n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4c674c9-3806-48f3-a002-d3c5cd1b5ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = LeakageTester(input_dim=768, num_classes=len(accents))  # linear probe\n",
    "opt = torch.optim.AdamW(lt.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "838575fd-900e-48f0-8d7f-adbb58c43c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] ACC: 0.9955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american     0.9933    1.0000    0.9967      1490\n",
      "  australian     0.9933    0.9926    0.9930      1490\n",
      "      bangla     0.9867    1.0000    0.9933      1560\n",
      "     british     1.0000    0.9993    0.9997      1490\n",
      "      indian     1.0000    1.0000    1.0000      1490\n",
      "   malayalam     1.0000    0.9934    0.9967      1510\n",
      "       odiya     1.0000    1.0000    1.0000      1500\n",
      "      telugu     1.0000    0.9812    0.9905      1540\n",
      "       welsh     0.9867    0.9933    0.9900      1490\n",
      "\n",
      "    accuracy                         0.9955     13560\n",
      "   macro avg     0.9956    0.9955    0.9955     13560\n",
      "weighted avg     0.9955    0.9955    0.9955     13560\n",
      "\n",
      "Confusion:\n",
      " [[1490    0    0    0    0    0    0    0    0]\n",
      " [  10 1479    0    0    0    0    0    0    1]\n",
      " [   0    0 1560    0    0    0    0    0    0]\n",
      " [   0    0    1 1489    0    0    0    0    0]\n",
      " [   0    0    0    0 1490    0    0    0    0]\n",
      " [   0    0   10    0    0 1500    0    0    0]\n",
      " [   0    0    0    0    0    0 1500    0    0]\n",
      " [   0    0   10    0    0    0    0 1511   19]\n",
      " [   0   10    0    0    0    0    0    0 1480]]\n",
      "Epoch 01 | train loss 0.2641\n",
      "[VAL] ACC: 0.9958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american     0.9933    1.0000    0.9967      1490\n",
      "  australian     0.9867    0.9926    0.9896      1490\n",
      "      bangla     0.9924    1.0000    0.9962      1560\n",
      "     british     1.0000    1.0000    1.0000      1490\n",
      "      indian     1.0000    0.9966    0.9983      1490\n",
      "   malayalam     0.9967    0.9993    0.9980      1510\n",
      "       odiya     1.0000    1.0000    1.0000      1500\n",
      "      telugu     1.0000    0.9870    0.9935      1540\n",
      "       welsh     0.9932    0.9866    0.9899      1490\n",
      "\n",
      "    accuracy                         0.9958     13560\n",
      "   macro avg     0.9958    0.9958    0.9958     13560\n",
      "weighted avg     0.9958    0.9958    0.9958     13560\n",
      "\n",
      "Confusion:\n",
      " [[1490    0    0    0    0    0    0    0    0]\n",
      " [  10 1479    1    0    0    0    0    0    0]\n",
      " [   0    0 1560    0    0    0    0    0    0]\n",
      " [   0    0    0 1490    0    0    0    0    0]\n",
      " [   0    0    0    0 1485    5    0    0    0]\n",
      " [   0    0    1    0    0 1509    0    0    0]\n",
      " [   0    0    0    0    0    0 1500    0    0]\n",
      " [   0    0   10    0    0    0    0 1520   10]\n",
      " [   0   20    0    0    0    0    0    0 1470]]\n",
      "Epoch 02 | train loss 0.0001\n",
      "[VAL] ACC: 0.9954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american     0.9933    1.0000    0.9967      1490\n",
      "  australian     0.9867    0.9926    0.9896      1490\n",
      "      bangla     0.9886    1.0000    0.9943      1560\n",
      "     british     1.0000    1.0000    1.0000      1490\n",
      "      indian     1.0000    0.9973    0.9987      1490\n",
      "   malayalam     0.9973    0.9954    0.9964      1510\n",
      "       odiya     1.0000    1.0000    1.0000      1500\n",
      "      telugu     1.0000    0.9870    0.9935      1540\n",
      "       welsh     0.9932    0.9866    0.9899      1490\n",
      "\n",
      "    accuracy                         0.9954     13560\n",
      "   macro avg     0.9955    0.9954    0.9954     13560\n",
      "weighted avg     0.9955    0.9954    0.9954     13560\n",
      "\n",
      "Confusion:\n",
      " [[1490    0    0    0    0    0    0    0    0]\n",
      " [  10 1479    1    0    0    0    0    0    0]\n",
      " [   0    0 1560    0    0    0    0    0    0]\n",
      " [   0    0    0 1490    0    0    0    0    0]\n",
      " [   0    0    0    0 1486    4    0    0    0]\n",
      " [   0    0    7    0    0 1503    0    0    0]\n",
      " [   0    0    0    0    0    0 1500    0    0]\n",
      " [   0    0   10    0    0    0    0 1520   10]\n",
      " [   0   20    0    0    0    0    0    0 1470]]\n",
      "Epoch 03 | train loss 0.0000\n",
      "[VAL] ACC: 0.9954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american     0.9933    1.0000    0.9967      1490\n",
      "  australian     0.9867    0.9926    0.9896      1490\n",
      "      bangla     0.9886    1.0000    0.9943      1560\n",
      "     british     1.0000    1.0000    1.0000      1490\n",
      "      indian     1.0000    0.9973    0.9987      1490\n",
      "   malayalam     0.9973    0.9954    0.9964      1510\n",
      "       odiya     1.0000    1.0000    1.0000      1500\n",
      "      telugu     1.0000    0.9870    0.9935      1540\n",
      "       welsh     0.9932    0.9866    0.9899      1490\n",
      "\n",
      "    accuracy                         0.9954     13560\n",
      "   macro avg     0.9955    0.9954    0.9954     13560\n",
      "weighted avg     0.9955    0.9954    0.9954     13560\n",
      "\n",
      "Confusion:\n",
      " [[1490    0    0    0    0    0    0    0    0]\n",
      " [  10 1479    1    0    0    0    0    0    0]\n",
      " [   0    0 1560    0    0    0    0    0    0]\n",
      " [   0    0    0 1490    0    0    0    0    0]\n",
      " [   0    0    0    0 1486    4    0    0    0]\n",
      " [   0    0    7    0    0 1503    0    0    0]\n",
      " [   0    0    0    0    0    0 1500    0    0]\n",
      " [   0    0   10    0    0    0    0 1520   10]\n",
      " [   0   20    0    0    0    0    0    0 1470]]\n",
      "Epoch 04 | train loss 0.0000\n",
      "[VAL] ACC: 0.9954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american     0.9933    1.0000    0.9967      1490\n",
      "  australian     0.9867    0.9926    0.9896      1490\n",
      "      bangla     0.9886    1.0000    0.9943      1560\n",
      "     british     1.0000    1.0000    1.0000      1490\n",
      "      indian     1.0000    0.9973    0.9987      1490\n",
      "   malayalam     0.9973    0.9954    0.9964      1510\n",
      "       odiya     1.0000    1.0000    1.0000      1500\n",
      "      telugu     1.0000    0.9870    0.9935      1540\n",
      "       welsh     0.9932    0.9866    0.9899      1490\n",
      "\n",
      "    accuracy                         0.9954     13560\n",
      "   macro avg     0.9955    0.9954    0.9954     13560\n",
      "weighted avg     0.9955    0.9954    0.9954     13560\n",
      "\n",
      "Confusion:\n",
      " [[1490    0    0    0    0    0    0    0    0]\n",
      " [  10 1479    1    0    0    0    0    0    0]\n",
      " [   0    0 1560    0    0    0    0    0    0]\n",
      " [   0    0    0 1490    0    0    0    0    0]\n",
      " [   0    0    0    0 1486    4    0    0    0]\n",
      " [   0    0    7    0    0 1503    0    0    0]\n",
      " [   0    0    0    0    0    0 1500    0    0]\n",
      " [   0    0   10    0    0    0    0 1520   10]\n",
      " [   0   20    0    0    0    0    0    0 1470]]\n",
      "Epoch 05 | train loss 0.0000\n",
      "[VAL] ACC: 0.9954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american     0.9933    1.0000    0.9967      1490\n",
      "  australian     0.9867    0.9926    0.9896      1490\n",
      "      bangla     0.9886    1.0000    0.9943      1560\n",
      "     british     1.0000    1.0000    1.0000      1490\n",
      "      indian     1.0000    0.9973    0.9987      1490\n",
      "   malayalam     0.9973    0.9954    0.9964      1510\n",
      "       odiya     1.0000    1.0000    1.0000      1500\n",
      "      telugu     1.0000    0.9870    0.9935      1540\n",
      "       welsh     0.9932    0.9866    0.9899      1490\n",
      "\n",
      "    accuracy                         0.9954     13560\n",
      "   macro avg     0.9955    0.9954    0.9954     13560\n",
      "weighted avg     0.9955    0.9954    0.9954     13560\n",
      "\n",
      "Confusion:\n",
      " [[1490    0    0    0    0    0    0    0    0]\n",
      " [  10 1479    1    0    0    0    0    0    0]\n",
      " [   0    0 1560    0    0    0    0    0    0]\n",
      " [   0    0    0 1490    0    0    0    0    0]\n",
      " [   0    0    0    0 1486    4    0    0    0]\n",
      " [   0    0    7    0    0 1503    0    0    0]\n",
      " [   0    0    0    0    0    0 1500    0    0]\n",
      " [   0    0   10    0    0    0    0 1520   10]\n",
      " [   0   20    0    0    0    0    0    0 1470]]\n",
      "Epoch 06 | train loss 0.0000\n",
      "[VAL] ACC: 0.9954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american     0.9933    1.0000    0.9967      1490\n",
      "  australian     0.9867    0.9926    0.9896      1490\n",
      "      bangla     0.9886    1.0000    0.9943      1560\n",
      "     british     1.0000    1.0000    1.0000      1490\n",
      "      indian     1.0000    0.9973    0.9987      1490\n",
      "   malayalam     0.9973    0.9954    0.9964      1510\n",
      "       odiya     1.0000    1.0000    1.0000      1500\n",
      "      telugu     1.0000    0.9870    0.9935      1540\n",
      "       welsh     0.9932    0.9866    0.9899      1490\n",
      "\n",
      "    accuracy                         0.9954     13560\n",
      "   macro avg     0.9955    0.9954    0.9954     13560\n",
      "weighted avg     0.9955    0.9954    0.9954     13560\n",
      "\n",
      "Confusion:\n",
      " [[1490    0    0    0    0    0    0    0    0]\n",
      " [  10 1479    1    0    0    0    0    0    0]\n",
      " [   0    0 1560    0    0    0    0    0    0]\n",
      " [   0    0    0 1490    0    0    0    0    0]\n",
      " [   0    0    0    0 1486    4    0    0    0]\n",
      " [   0    0    7    0    0 1503    0    0    0]\n",
      " [   0    0    0    0    0    0 1500    0    0]\n",
      " [   0    0   10    0    0    0    0 1520   10]\n",
      " [   0   20    0    0    0    0    0    0 1470]]\n",
      "Epoch 07 | train loss 0.0000\n",
      "[VAL] ACC: 0.9954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american     0.9933    1.0000    0.9967      1490\n",
      "  australian     0.9867    0.9926    0.9896      1490\n",
      "      bangla     0.9886    1.0000    0.9943      1560\n",
      "     british     1.0000    1.0000    1.0000      1490\n",
      "      indian     1.0000    0.9973    0.9987      1490\n",
      "   malayalam     0.9973    0.9954    0.9964      1510\n",
      "       odiya     1.0000    1.0000    1.0000      1500\n",
      "      telugu     1.0000    0.9870    0.9935      1540\n",
      "       welsh     0.9932    0.9866    0.9899      1490\n",
      "\n",
      "    accuracy                         0.9954     13560\n",
      "   macro avg     0.9955    0.9954    0.9954     13560\n",
      "weighted avg     0.9955    0.9954    0.9954     13560\n",
      "\n",
      "Confusion:\n",
      " [[1490    0    0    0    0    0    0    0    0]\n",
      " [  10 1479    1    0    0    0    0    0    0]\n",
      " [   0    0 1560    0    0    0    0    0    0]\n",
      " [   0    0    0 1490    0    0    0    0    0]\n",
      " [   0    0    0    0 1486    4    0    0    0]\n",
      " [   0    0    7    0    0 1503    0    0    0]\n",
      " [   0    0    0    0    0    0 1500    0    0]\n",
      " [   0    0   10    0    0    0    0 1520   10]\n",
      " [   0   20    0    0    0    0    0    0 1470]]\n",
      "Epoch 08 | train loss 0.0000\n",
      "[VAL] ACC: 0.9955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american     0.9933    1.0000    0.9967      1490\n",
      "  australian     0.9867    0.9933    0.9900      1490\n",
      "      bangla     0.9892    1.0000    0.9946      1560\n",
      "     british     1.0000    1.0000    1.0000      1490\n",
      "      indian     1.0000    0.9973    0.9987      1490\n",
      "   malayalam     0.9973    0.9954    0.9964      1510\n",
      "       odiya     1.0000    1.0000    1.0000      1500\n",
      "      telugu     1.0000    0.9870    0.9935      1540\n",
      "       welsh     0.9932    0.9866    0.9899      1490\n",
      "\n",
      "    accuracy                         0.9955     13560\n",
      "   macro avg     0.9955    0.9955    0.9955     13560\n",
      "weighted avg     0.9955    0.9955    0.9955     13560\n",
      "\n",
      "Confusion:\n",
      " [[1490    0    0    0    0    0    0    0    0]\n",
      " [  10 1480    0    0    0    0    0    0    0]\n",
      " [   0    0 1560    0    0    0    0    0    0]\n",
      " [   0    0    0 1490    0    0    0    0    0]\n",
      " [   0    0    0    0 1486    4    0    0    0]\n",
      " [   0    0    7    0    0 1503    0    0    0]\n",
      " [   0    0    0    0    0    0 1500    0    0]\n",
      " [   0    0   10    0    0    0    0 1520   10]\n",
      " [   0   20    0    0    0    0    0    0 1470]]\n",
      "Epoch 09 | train loss 0.0000\n",
      "[VAL] ACC: 0.9955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american     0.9933    1.0000    0.9967      1490\n",
      "  australian     0.9867    0.9933    0.9900      1490\n",
      "      bangla     0.9892    1.0000    0.9946      1560\n",
      "     british     1.0000    1.0000    1.0000      1490\n",
      "      indian     1.0000    0.9973    0.9987      1490\n",
      "   malayalam     0.9973    0.9954    0.9964      1510\n",
      "       odiya     1.0000    1.0000    1.0000      1500\n",
      "      telugu     1.0000    0.9870    0.9935      1540\n",
      "       welsh     0.9932    0.9866    0.9899      1490\n",
      "\n",
      "    accuracy                         0.9955     13560\n",
      "   macro avg     0.9955    0.9955    0.9955     13560\n",
      "weighted avg     0.9955    0.9955    0.9955     13560\n",
      "\n",
      "Confusion:\n",
      " [[1490    0    0    0    0    0    0    0    0]\n",
      " [  10 1480    0    0    0    0    0    0    0]\n",
      " [   0    0 1560    0    0    0    0    0    0]\n",
      " [   0    0    0 1490    0    0    0    0    0]\n",
      " [   0    0    0    0 1486    4    0    0    0]\n",
      " [   0    0    7    0    0 1503    0    0    0]\n",
      " [   0    0    0    0    0    0 1500    0    0]\n",
      " [   0    0   10    0    0    0    0 1520   10]\n",
      " [   0   20    0    0    0    0    0    0 1470]]\n",
      "Epoch 10 | train loss 0.0000\n"
     ]
    }
   ],
   "source": [
    "best, state = 0.0, None\n",
    "for ep in range(10):\n",
    "    #y_train = torch.from_numpy(np.random.permutation(y_train))\n",
    "    tr_loss = train_epoch(lt, opt, train_frame_features, train_frame_labels)\n",
    "    val_acc = evaluate(lt, validation_frame_features, validation_frame_labels, accents, \"VAL\")\n",
    "    print(f\"Epoch {ep+1:02d} | train loss {tr_loss:.4f}\")\n",
    "    if val_acc > best: best, state = val_acc, {k:v.cpu() for k,v in lt.state_dict().items()}\n",
    "if state: lt.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2ab74d9-08ec-4ebf-a058-3533f2a7f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST ===\n",
      "[TEST] ACC: 0.1316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american     0.1945    0.1208    0.1490      7420\n",
      "  australian     0.0159    0.0101    0.0124      7420\n",
      "      bangla     0.1904    0.1061    0.1363      7500\n",
      "     british     0.3780    0.3361    0.3558      7420\n",
      "      indian     0.2525    0.2838    0.2673      7420\n",
      "   malayalam     0.0153    0.0155    0.0154      7470\n",
      "       odiya     0.0000    0.0000    0.0000         0\n",
      "      telugu     0.1497    0.0507    0.0758      7490\n",
      "       welsh     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.1316     52140\n",
      "   macro avg     0.1329    0.1026    0.1124     52140\n",
      "weighted avg     0.1708    0.1316    0.1443     52140\n",
      "\n",
      "Confusion:\n",
      " [[ 896 3502  470  266  178  293 1053   44  718]\n",
      " [ 442   75  640 1704  537 2412   10  807  793]\n",
      " [ 108  111  796  463 3121  318 2110  355  118]\n",
      " [1626  515  362 2494  182  463 1062  701   15]\n",
      " [  19  293    4  150 2106  942 3463  230  213]\n",
      " [  93  140  420  668 2146  116 3843   21   23]\n",
      " [   0    0    0    0    0    0    0    0    0]\n",
      " [1423   79 1489  853   69 3014    9  380  174]\n",
      " [   0    0    0    0    0    0    0    0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjun/miniconda3/envs/contentvec/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/arjun/miniconda3/envs/contentvec/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/arjun/miniconda3/envs/contentvec/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13162639048714997"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== TEST ===\")\n",
    "evaluate(lt, test_frame_features, test_frame_labels, accents, \"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e32c1417-5650-4308-b680-a596d3294b7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnear_duplicate_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_frame_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_frame_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Lab/cs5647-labs/project_cv/content_encoder_utils.py:116\u001b[0m, in \u001b[0;36mnear_duplicate_pairs\u001b[0;34m(Xtr, Xte, topk, cos_thr)\u001b[0m\n\u001b[1;32m    114\u001b[0m Xt \u001b[38;5;241m=\u001b[39m Xtr\u001b[38;5;66;03m#torch.tensor(Xtr, dtype=torch.float32)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m Xe \u001b[38;5;241m=\u001b[39m Xte\u001b[38;5;66;03m#torch.tensor(Xte, dtype=torch.float32)\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39mnormalize(Xt, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m); Xe \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(Xe, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    117\u001b[0m sim \u001b[38;5;241m=\u001b[39m Xe \u001b[38;5;241m@\u001b[39m Xt\u001b[38;5;241m.\u001b[39mT                       \u001b[38;5;66;03m# [Nte, Ntr]\u001b[39;00m\n\u001b[1;32m    118\u001b[0m max_sim, idx \u001b[38;5;241m=\u001b[39m sim\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "utils.near_duplicate_pairs(train_frame_features[:20000], validation_frame_features[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3886330c-d4b4-49fe-83af-e2d2a66eb6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27695 3462\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
